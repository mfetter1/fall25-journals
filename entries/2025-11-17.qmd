---
title: "2025-11-17"
format:
  html: default
  pdf: default
params:
  course: "mc501"
  word_min: 450
  word_max: 500
  p1: 'This chapter describes inference as a “leap” from sample to population. Reflect on what makes that leap trustworthy—or risky. Why is it not enough to observe a pattern in your sample? How does hypothesis testing help, and what limits remain even when your results are statistically significant?'
  p2: 'Many people misunderstand the p-value as “proof.” Why is this incorrect? What does a small p-value tell us—and what does it not tell us? Reflect on a time you saw a research claim or news headline that leaned too heavily on the idea of “significance.” What might have been missing?'
  p3: 'Imagine you find a statistically significant result in your research—but the effect size is tiny. Would you still report it? Why or why not? How do you balance statistical significance with practical or social importance? What responsibility do researchers have when communicating findings that might be misinterpreted?'
---

## Choose **one** prompt to answer

> **Prompt A:** `r params$p1`

---

## Response

<!-- RESPONSE-START -->In this chapter, inference is described as a “leap” from a sample to a population, and hypothesis testing offers the structure that helps make that leap more trustworthy. However, even with statistical tools, the leap is never risk-free. Understanding the why requires examining both how hypotheses and testing work and what their limits are. 

A pattern observed in a sample is never enough on its own because samples are imperfect and representations of the population. Random chance, sampling error, and uneven group assignments can all produce differences that appear meaningful but actually reflect noise rather than a genuine effect. Without a formal way to evaluate how probable a sample result is under the assumption of no real relationship, researchers risk mistaking coincidence for evidence. This is why simply seeing a difference like a 10-point test score increase by the treatment group and the control group. This is not as compelling on its own. We need to know whether the difference could easily appear even if the treatment had no actual effect. 

Testing a hypothesis by introducing a cautious process that begins with assuming the null hypothesis is true. This forces researchers to evaluate how surprising their data would be if there really were no relationship in the population. The p-value tells us that if the probability is very small, the result is statistically significant, and we reject the null hypothesis. This does not prove the research hypothesis, but it does provide evidence that the observed pattern is unlikely to be due to random chance alone.

The chapter also talks about how hypothesis testing protects against two types of errors. Type I errors occur when we reject a true null hypothesis, while type II errors occur when we fail to detect a real effect. These errors highlight the unavoidable risks of drawing conclusions from incomplete information. Even when a result is statistically significant, there is still a chance of a 5% that we are wrong. Recognizing these risks can help make the leap from sample to population more careful and transparent. Even with hypothesis testing, there are still important limits. Statistical significance only addresses whether an effect is likely real; it says nothing about how large or meaningful that effect is. With large samples, even trivial differences can become statistically significant. The chapter underscores the importance of effect size as a measure of practical importance. This is something that hypothesis testing alone cannot provide. A tiny effect may be real but irrelevant, and reporting only significance levels can mislead readers into overestimating the impact of a finding. Finally, statistical significance cannot address issues like biased samples. Poor measurements or confounding variables. Even a well-executed test does not establish causation or guarantee that results generalize beyond the sample.<!-- RESPONSE-END -->

---

## Word Count & Range Check

```{r}
#| echo: false
#| message: false
#| warning: false
get_response_text <- function() {
  f <- knitr::current_input()
  if (is.null(f) || !file.exists(f)) return("")
  x <- readLines(f, warn = FALSE)
  # Find the lines that EXACTLY match the start/end markers
  s <- grep("^<!-- RESPONSE-START -->$", x)
  e <- grep("^<!-- RESPONSE-END -->$", x)
  if (length(s) != 1 || length(e) != 1 || e <= s) return("")
  paste(x[(s + 1L):(e - 1L)], collapse = "\n")
}
count_words <- function(txt) {
  # Remove code blocks and inline code before counting
  txt <- gsub("```[\\s\\S]*?```", " ", txt, perl = TRUE)
  txt <- gsub("`[^`]*`", " ", txt, perl = TRUE)
  # Keep letters, numbers, spaces, hyphens, and apostrophes
  txt <- gsub("[^\\p{L}\\p{N}\\s'-]", " ", txt, perl = TRUE)
  # Split by whitespace and count non-empty words
  words <- unlist(strsplit(txt, "\\s+", perl = TRUE))
  words <- words[nzchar(words)]
  length(words)
}
txt <- get_response_text()
n <- count_words(txt)
minw <- as.integer(params$word_min)
maxw <- as.integer(params$word_max)
in_range <- n >= minw && n <= maxw
cat(sprintf("**Word count:** %d  \n", n))
cat(sprintf("**Required range (%s):** %d–%d words  \n",
            toupper(params$course), minw, maxw))
cat(if (in_range) "**Status:** ✅ In range\n" else "**Status:** ❌ Out of range\n")
```
